# PoliticAI System Architecture - Complete Technical Explanation

## Executive Overview

PoliticAI is a **sentiment analysis and intelligence platform** designed to track political news, analyze sentiment, compute real-time scores, and deliver actionable insights. The system processes news from multiple sources, enriches them with sentiment/confidence metrics, and aggregates data into daily statistics and pulse scores for geographic units, candidates, and parties.

---

## ğŸ—ï¸ System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND (React)                         â”‚
â”‚          (Dashboard, Admin Panel, Analysis Tools)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ NextJS â”‚  â”‚ NestJS   â”‚  â”‚  FastAPI  â”‚
    â”‚ Backendâ”‚  â”‚ Backend  â”‚  â”‚  Analysis â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚ Service   â”‚
         â”‚             â”‚       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”
    â”‚PostgreSQL â”‚  â”‚ Prisma â”‚  â”‚  Python  â”‚
    â”‚ Database  â”‚  â”‚ Client â”‚  â”‚  Models  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ElectionData | News | Sentiment     â”‚
    â”‚   GeoStats | Candidates | Alerts      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“° 1. NEWS INGESTION SYSTEM

### 1.1 How News Gets Into The System

The news ingestion system fetches news articles from Google News RSS and automatically links them to active political entities. It's driven by `EntityMonitoring` (which tracks what entities to monitor) and `NewsKeyword` (which stores keywords for each entity).

#### Political Entities Definition
**Political Entities** are the 3 actor types in the system:
- **CANDIDATE** - Individual politicians (e.g., Siddaramaiah, Krishna)
- **PARTY** - Political organizations (e.g., Congress, BJP, JDS)
- **GEO_UNIT** - Geographic regions at various levels (State, District, Constituency, Ward, Booth)

#### Flow Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  NEWS INGESTION FLOW (UPDATED)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 0: ENTITY ACTIVATION (Setup)
â”œâ”€ User subscribes to: Siddaramaiah (Candidate)
â”‚  â””â”€ CandidateProfile {candidateId: 123, isSubscribed: true}
â”‚
â”œâ”€ System auto-creates EntityMonitoring records:
â”‚  â”œâ”€ {entityType: CANDIDATE, entityId: 123, reason: "SUBSCRIBED"}
â”‚  â”œâ”€ {entityType: PARTY, entityId: 789, reason: "PARTY_CONTEXT"}
â”‚  â”œâ”€ {entityType: GEO_UNIT, entityId: 456, reason: "GEO_CONTEXT"}
â”‚  â””â”€ {entityType: CANDIDATE, entityId: 124, reason: "OPPONENT"}
â”‚
â””â”€ For each entity, NewsKeyword entries are auto-generated:
   â”œâ”€ Candidate 123: ["Siddaramaiah", "Siddu", "Karnataka CM"]
   â”œâ”€ Party 789: ["Congress", "INC"]
   â””â”€ GeoUnit 456: ["Bangalore", "Bangalore Constituency"]

STEP 1: HOURLY INGESTION JOB (Every hour)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT * FROM EntityMonitoring WHERE isActive=true  â”‚
â”‚ (Find all candidates, parties, geounits to fetch)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ FOR EACH ACTIVE ENTITY:                              â”‚
      â”‚ â€¢ Look up keywords in NewsKeyword table              â”‚
      â”‚ â€¢ Build Google News RSS query:                       â”‚
      â”‚   ("Keyword1" OR "Keyword2") AND                     â”‚
      â”‚   (election OR vote OR campaign OR policy...)        â”‚
      â”‚ â€¢ Query Google News API                             â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ DEDUPLICATION & ARTICLE CREATION                      â”‚
      â”‚                                                        â”‚
      â”‚ For each RSS item:                                    â”‚
      â”‚ â€¢ Extract: title, summary, source, publishedAt, link â”‚
      â”‚ â€¢ Check if sourceUrl already in NewsArticle table     â”‚
      â”‚ â€¢ If EXISTS:                                          â”‚
      â”‚   â””â”€ Skip (already processed)                         â”‚
      â”‚ â€¢ If NEW:                                             â”‚
      â”‚   â”œâ”€ Create NewsArticle {status: APPROVED}            â”‚
      â”‚   â””â”€ Create NewsEntityMention entries                â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ CREATE ENTITY MENTION LINKS (Multiple per article)    â”‚
      â”‚                                                        â”‚
      â”‚ For article: "Siddaramaiah announces scheme in         â”‚
      â”‚              Bangalore"                                â”‚
      â”‚                                                        â”‚
      â”‚ Create NewsEntityMention records:                      â”‚
      â”‚ â”œâ”€ {articleId: 5001, entityType: CANDIDATE,            â”‚
      â”‚ â”‚   entityId: 123, relevanceWeight: 1.0}              â”‚
      â”‚ â”œâ”€ {articleId: 5001, entityType: GEO_UNIT,            â”‚
      â”‚ â”‚   entityId: 456, relevanceWeight: 0.85}             â”‚
      â”‚ â””â”€ {articleId: 5001, entityType: PARTY,               â”‚
      â”‚     entityId: 789, relevanceWeight: 0.70}             â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ TRIGGER SENTIMENT ANALYSIS (Async, Non-blocking)      â”‚
      â”‚                                                        â”‚
      â”‚ For each new article:                                 â”‚
      â”‚ â€¢ Pass to Python BERT service (async queue)           â”‚
      â”‚ â€¢ Service analyzes text sentiment                      â”‚
      â”‚ â€¢ Creates SentimentSignal record(s)                    â”‚
      â”‚ â€¢ Errors don't block ingestion                        â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 News Fetching Strategy

#### Questions Answered:
**Q: Does it fetch only from NewsKeyword table?**
**A:** No. NewsKeyword is the lookup table. The system fetches for all entities in EntityMonitoring (isActive=true).

**Q: Does it fetch for subscribed candidates only?**
**A:** No. It fetches for:
- âœ… Subscribed candidates (reason="SUBSCRIBED")
- âœ… Their opposition candidates (reason="OPPONENT")
- âœ… Candidate's party (reason="PARTY_CONTEXT")
- âœ… Candidate's constituency/state (reason="GEO_CONTEXT")

**Q: Should we fetch for all geounits?**
**A:** Yes, but with priority tiers (Currently: All entities fetched equally every hour. Priority-based scheduling not yet implemented).

```
PRIORITY LEVELS (To Be Implemented):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TIER 1 (HIGH PRIORITY - Fetch every 1 hour):
â”œâ”€ Subscribed candidate (priority: 10)
â”œâ”€ Candidate's primary constituency (priority: 9)
â”œâ”€ Opposition candidates in same constituency (priority: 9)
â””â”€ Candidate's party (priority: 8)

TIER 2 (MEDIUM PRIORITY - Fetch every 2 hours):
â”œâ”€ Parent district (priority: 6)
â”œâ”€ Adjacent constituencies (priority: 5)
â””â”€ Regional political news (priority: 5)

TIER 3 (LOW PRIORITY - Fetch every 6 hours):
â”œâ”€ Parent state (priority: 3)
â”œâ”€ Other states news (priority: 2)
â””â”€ National political news (priority: 1)

Implementation Strategy (Not Yet Implemented):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NewsKeyword table:
â”œâ”€ {entityType: CANDIDATE, entityId: 123, priority: 10}  â† Tier 1
â”œâ”€ {entityType: PARTY, entityId: 789, priority: 8}       â† Tier 1
â”œâ”€ {entityType: GEO_UNIT, entityId: 456, priority: 9}    â† Tier 1 (constituency)
â”œâ”€ {entityType: GEO_UNIT, entityId: 789, priority: 6}    â† Tier 2 (district)
â””â”€ {entityType: GEO_UNIT, entityId: 1, priority: 3}      â† Tier 3 (state)

Scheduler (To Implement):
â”œâ”€ NewsIngestionScheduler_Tier1 @Cron('0 * * * * *')
â”‚  â””â”€ Run EVERY HOUR: Fetch all keywords with priority >= 9
â”‚
â”œâ”€ NewsIngestionScheduler_Tier2 @Cron('0 */2 * * * *')
â”‚  â””â”€ Run EVERY 2 HOURS: Fetch keywords with priority 5-8
â”‚
â””â”€ NewsIngestionScheduler_Tier3 @Cron('0 */6 * * * *')
   â””â”€ Run EVERY 6 HOURS: Fetch keywords with priority < 5

CURRENT STATUS: All entities fetched with same priority (every hour)
RECOMMENDATION: Implement tiered scheduling for efficiency
```

#### B. **EntityMonitoring-Driven Approach**
- **What drives fetching**: `EntityMonitoring` table (not CandidateProfile alone)
- **Why**: Allows tracking competitors, party context, and geographic context separately
- **Example**:
  ```
  User subscribes to: Siddaramaiah
  â”œâ”€ Creates EntityMonitoring {entityType: CANDIDATE, entityId: 123, reason: SUBSCRIBED}
  â”œâ”€ Creates EntityMonitoring {entityType: PARTY, entityId: 789, reason: PARTY_CONTEXT}
  â”œâ”€ Creates EntityMonitoring {entityType: GEO_UNIT, entityId: 456, reason: GEO_CONTEXT}
  â”œâ”€ Creates EntityMonitoring {entityType: CANDIDATE, entityId: 124, reason: OPPONENT}
  â””â”€ Creates EntityMonitoring {entityType: CANDIDATE, entityId: 125, reason: OPPONENT}
  
  News Fetching:
  â””â”€ Queries ALL these entities, not just Siddaramaiah
  ```

#### C. **KeywordManagerService**
- **Responsibility**: Build search queries for news fetching
- **How it works**:
  ```
  Entity: "Siddaramaiah" (Candidate)
          â†“
  Step 1: Look up NewsKeyword records
          â”œâ”€ keyword: "Siddaramaiah"
          â”œâ”€ keyword: "Siddu"
          â””â”€ keyword: "Karnataka CM"
          â†“
  Step 2: Add context terms
          â””â”€ (election OR vote OR campaign OR policy OR scandal)
          â†“
  Step 3: Build query
          Final: ("Siddaramaiah" OR "Siddu" OR "Karnataka CM") 
                 AND (election OR vote OR campaign OR policy OR scandal)
          â†“
  Step 4: Query Google News RSS
          â””â”€ Get latest articles (deduped by sourceUrl)
  ```

#### D. **NewsIngestionService** (The Scheduler)
- **Trigger**: `@Cron(CronExpression.EVERY_HOUR)`
- **What it does each hour**:
  ```
  1. Query EntityMonitoring WHERE isActive=true
     â””â”€ Get all candidates, parties, geounits to monitor
  
  2. FOR EACH active entity:
     â”œâ”€ Get keywords from NewsKeyword table
     â”œâ”€ Build Google News RSS query
     â”œâ”€ Query Google News API
     â”œâ”€ Parse RSS items
     â”‚
     â””â”€ FOR EACH RSS item:
        â”œâ”€ Extract: title, summary, source, publishedAt, link
        â”œâ”€ Check dedup: Does sourceUrl exist in NewsArticle?
        â”‚
        â”œâ”€ IF NEW article:
        â”‚  â”œâ”€ Create NewsArticle {status: APPROVED, ingestType: API}
        â”‚  â”œâ”€ Create NewsEntityMention entries for EACH entity type
        â”‚  â”‚  (CANDIDATE, PARTY, GEO_UNIT - all that were mentioned)
        â”‚  â””â”€ ASYNC TRIGGER: sentimentService.analyzeAndStoreSentiment()
        â”‚
        â””â”€ IF DUPLICATE:
           â””â”€ Skip (already in system)
  
  3. Log metrics:
     {entities_processed: 142, articles_fetched: 47, new_articles: 12, duration: "3.2s"}
  ```

#### E. **Execution Order: Ingestion â†’ Sentiment**
```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
10:00 AM â”€â”¬â”€â†’ NewsIngestionService.fetchAllNews() [SYNCHRONOUS, BLOCKING]
          â”‚    â””â”€ Runs to completion (~5 seconds)
          â”‚       Result: Articles saved in DB
          â”‚
          â”œâ”€â†’ Meanwhile: Triggers sentiment jobs (ASYNC)
          â”‚    â””â”€ sentimentService.analyzeAndStoreSentiment(articleId)
          â”‚       Result: Queued for background processing (~30-60 seconds)
          â”‚
          â””â”€ User sees articles immediately (before sentiment completes)
             (sentimentScore is NULL until sentiment job finishes)

This design ensures:
â”œâ”€ News ingestion is fast (doesn't wait for ML processing)
â”œâ”€ Sentiment analysis doesn't block news fetching
â””â”€ Users see latest articles immediately
```

### 1.3 Database Schema - News Tables

```sql
-- NewsArticle table
CREATE TABLE NewsArticle {
  id: Int @id                    -- Unique ID
  title: String                  -- "Election results: Karnataka..."
  summary: String @db.Text       -- Article excerpt
  sourceName: String             -- "The Hindu", "Google News", etc.
  sourceUrl: String              -- Full URL (unique)
  publishedAt: DateTime          -- Original publication time
  
  status: ModerationStatus       -- PENDING | APPROVED | REJECTED
  ingestType: NewsIngestType     -- API | SCRAPER | MANUAL | PARTNER
  
  sentimentSignals: [SentimentSignal]    -- Reverse relation
  entityMentions: [NewsEntityMention]    -- Which entities mentioned
  
  createdAt: DateTime            -- When ingested into PoliticAI
  updatedAt: DateTime
  
  @@index([status])              -- For moderation workflow
  @@index([publishedAt])         -- For sorting news
}

-- EntityMention: Maps articles to political entities
CREATE TABLE NewsEntityMention {
  id: Int @id
  articleId: Int                 -- Foreign key
  entityType: EntityType         -- CANDIDATE | PARTY | GEO_UNIT
  entityId: Int                  -- ID of the entity
  
  article: NewsArticle           -- Relation
  @@index([articleId])           -- Fast article lookup
  @@index([entityType, entityId])-- Fast entity lookup
}
```

### 1.4 News Ingestion Flow Sequence

```
TIME EXAMPLE: 10:00 AM IST (Scheduled Cron)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

10:00 AM â†’ NewsIngestionService.fetchAllNews()
   â”‚
   â”œâ”€â†’ Select all Candidates
   â”‚     â””â”€â†’ FOR "Siddaramaiah" (id=123)
   â”‚         â””â”€â†’ buildSearchQuery("Siddaramaiah")
   â”‚             â†’ Returns: ("Siddaramaiah" OR "Siddu") AND (election OR...)
   â”‚         â””â”€â†’ Query Google News RSS
   â”‚             â†’ 5 new articles found
   â”‚         â””â”€â†’ FOR each article:
   â”‚             â”œâ”€â†’ Check if URL exists â†’ NO
   â”‚             â”œâ”€â†’ Create NewsArticle (status=APPROVED)
   â”‚             â”œâ”€â†’ Create NewsEntityMention {
   â”‚             â”‚   articleId: 5001,
   â”‚             â”‚   entityType: "CANDIDATE",
   â”‚             â”‚   entityId: 123
   â”‚             â”‚ }
   â”‚             â””â”€â†’ ASYNC: sentimentService.analyze(5001, text)
   â”‚
   â”œâ”€â†’ Select all State-level GeoUnits
   â”‚     â””â”€â†’ FOR "Karnataka" (id=456)
   â”‚         â””â”€â†’ Repeat process
   â”‚
   â””â”€â†’ Select all Parties
         â””â”€â†’ FOR "Congress" (id=789)
             â””â”€â†’ Repeat process

10:05 AM â†’ All articles ingested
         â†’ Sentiment analysis jobs running in parallel
```

---

## ğŸ§  2. SENTIMENT ANALYSIS SYSTEM

### 2.1 How Sentiment Scoring Works

The system uses a **BERT-based sentiment model** that analyzes text and outputs 5 probability scores (for 1-star through 5-star sentiment). These are converted into a normalized score and confidence metric.

#### The BERT Model Pipeline
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SENTIMENT ANALYSIS PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: News Article Text
  Example: "Siddaramaiah announces new development policy for Bangalore"
  
  â”‚
  â”œâ”€â–º STEP 1: Language Detection
  â”‚   â””â”€â†’ Detects: "en" (English)
  â”‚
  â”œâ”€â–º STEP 2: BERT Model Inference
  â”‚   â”œâ”€â†’ Model: Hugging Face transformer (pre-trained on sentiment)
  â”‚   â”‚   (NOT hand-crafted rules - learned from millions of examples)
  â”‚   â”œâ”€â†’ Input: Full article text â†’ tokenized
  â”‚   â”‚   "Siddaramaiah" â†’ ["S", "iddara", "mai", "ah"] (subword tokens)
  â”‚   â”œâ”€â†’ Processing: 12-layer transformer with self-attention
  â”‚   â”‚   (learns context: what words mean together)
  â”‚   â””â”€â†’ Output: 5 probability values
  â”‚
  â”‚   Output: [prob_1star, prob_2star, prob_3star, prob_4star, prob_5star]
  â”‚   Example: [0.02,      0.05,      0.10,      0.45,      0.38]
  â”‚            (2%)        (5%)       (10%)       (45%)      (38%)
  â”‚
  â”‚            Interpretation:
  â”‚            â”œâ”€ 2% chance this is 1-star (very negative)
  â”‚            â”œâ”€ 5% chance this is 2-star (negative)
  â”‚            â”œâ”€ 10% chance this is 3-star (neutral)
  â”‚            â”œâ”€ 45% chance this is 4-star (positive) â† Most likely
  â”‚            â””â”€ 38% chance this is 5-star (very positive)
  â”‚
  â”œâ”€â–º STEP 3: Calculate Normalized Score (-1.0 to +1.0)
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Why normalize? 
  â”‚   â”‚   â€¢ Makes scores comparable across articles
  â”‚   â”‚   â€¢ Easier to average multiple articles
  â”‚   â”‚   â€¢ Captures both positive AND negative sentiment
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Formula: sentimentScore = Î£(probability[i] Ã— weight[i])
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Weights (represent intensity of sentiment):
  â”‚   â”‚   â€¢ 1 star  â†’ weight = -1.0  (extreme negative)
  â”‚   â”‚   â€¢ 2 stars â†’ weight = -0.5  (negative)
  â”‚   â”‚   â€¢ 3 stars â†’ weight = 0.0   (neutral/balanced)
  â”‚   â”‚   â€¢ 4 stars â†’ weight = +0.5  (positive)
  â”‚   â”‚   â€¢ 5 stars â†’ weight = +1.0  (extreme positive)
  â”‚   â”‚
  â”‚   â””â”€â†’ Calculation Example:
  â”‚       score = (0.02 Ã— -1.0) + (0.05 Ã— -0.5) + (0.10 Ã— 0.0) 
  â”‚              + (0.45 Ã— 0.5) + (0.38 Ã— 1.0)
  â”‚            = -0.02 + (-0.025) + 0 + 0.225 + 0.38
  â”‚            = 0.558 (55.8% positive)
  â”‚
  â”œâ”€â–º STEP 4: Determine Primary Sentiment Label
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Step 4a: Find the maximum probability
  â”‚   â”‚   [0.02, 0.05, 0.10, 0.45, 0.38]
  â”‚   â”‚    â†‘     â†‘     â†‘     â†‘     â†‘
  â”‚   â”‚   1â˜…    2â˜…    3â˜…    4â˜…    5â˜…
  â”‚   â”‚                      â†‘
  â”‚   â”‚                    MAX = 0.45 at index [3]
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Step 4b: Map index to sentiment class
  â”‚   â”‚   â€¢ Index [0] or [1] â†’ "NEGATIVE" (1-2 stars)
  â”‚   â”‚   â€¢ Index [2] â†’ "NEUTRAL" (3 stars)
  â”‚   â”‚   â€¢ Index [3] or [4] â†’ "POSITIVE" (4-5 stars)
  â”‚   â”‚
  â”‚   â””â”€â†’ Result: Label = "POSITIVE" (because max is at index 3 = 4-star)
  â”‚
  â”œâ”€â–º STEP 5: Calculate Confidence Score
  â”‚   â”‚
  â”‚   â”œâ”€â†’ What is confidence?
  â”‚   â”‚   = The highest probability value (model's certainty)
  â”‚   â”‚   = How sure the model is about its prediction
  â”‚   â”‚
  â”‚   â””â”€â†’ Calculation: confidence = max([0.02, 0.05, 0.10, 0.45, 0.38])
  â”‚                              = 0.45 (45%)
  â”‚
  â”‚       Interpretation:
  â”‚       â”œâ”€ 0.45 = Model is 45% confident it's 4-star positive
  â”‚       â”œâ”€ High confidence (0.85+) = Very reliable prediction
  â”‚       â”œâ”€ Medium confidence (0.50-0.84) = Somewhat reliable
  â”‚       â””â”€ Low confidence (<0.50) = Model is confused/uncertain
  â”‚
  â””â”€â–º OUTPUT: SentimentResponse
      {
        label: "POSITIVE",           â† Primary sentiment (POSITIVE/NEUTRAL/NEGATIVE)
        score: 0.558,                â† Normalized score (-1.0 to +1.0)
        confidence: 0.45,            â† Model certainty (0.0 to 1.0)
        model_version: "kn-en-v1",   â† For audit trail
        language: "en"               â† Detected language
      }
```

### 2.2 Understanding Confidence - Examples

```
EXAMPLE 1: HIGH CONFIDENCE (Very Reliable)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Article: "Candidate wins major election by landslide!"
BERT Output: [0.01, 0.02, 0.02, 0.05, 0.90]
                                         â†‘
Confidence: 0.90 (90%)
Label: POSITIVE (5-star has max probability)

Interpretation:
â”œâ”€ "Model is 90% sure this is 5-star positive"
â”œâ”€ "Very strong signal - highly reliable"
â””â”€ Use FULL weight in pulse calculations

EXAMPLE 2: MEDIUM CONFIDENCE (Moderately Reliable)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Article: "Candidate announces new policy"
BERT Output: [0.02, 0.05, 0.10, 0.45, 0.38]
                                â†‘
Confidence: 0.45 (45%)
Label: POSITIVE (4-star has max probability)

Interpretation:
â”œâ”€ "Model thinks it's 4-star (45% likely)"
â”œâ”€ "But there's significant uncertainty (38% chance it's 5-star)"
â”œâ”€ "Somewhat reliable but not certain"
â””â”€ Use REDUCED weight (multiply by 0.45) in calculations

EXAMPLE 3: LOW CONFIDENCE (Unreliable - Confused)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Article: "Political analysis report"
BERT Output: [0.20, 0.20, 0.25, 0.20, 0.15]
             â†‘     â†‘     â†‘     â†‘     â†‘
Confidence: 0.25 (25%)
Label: NEUTRAL (3-star has max probability)

Interpretation:
â”œâ”€ "Model is confused - almost equal probabilities"
â”œâ”€ "Almost same chance of any sentiment"
â”œâ”€ "Very unreliable prediction"
â”œâ”€ Use MINIMAL weight or exclude entirely
â””â”€ Flag for manual review

WHY CONFIDENCE MATTERS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Don't multiply by confidence? 
  Problem: Can't distinguish between:
    â”œâ”€ Confident +0.8: Should count as 0.8
    â””â”€ Unconfident +0.8: Should count as less (maybe 0.25)
    
By multiplying:
  Confident +0.8:   0.8 Ã— 0.90 = 0.72 âœ“ (use most of it)
  Unconfident +0.8: 0.8 Ã— 0.25 = 0.20 âœ“ (reduce drastically)
```

### 2.3 Why We Normalize Scores to -1.0 to +1.0

```
PROBLEM WITH RAW PROBABILITIES [0.0, 1.0]:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Example: Articles about Siddaramaiah

Article 1: [0.02, 0.05, 0.10, 0.45, 0.38] â†’ "Positive"
Article 2: [0.20, 0.20, 0.25, 0.20, 0.15] â†’ "Neutral"
Article 3: [0.85, 0.10, 0.02, 0.02, 0.01] â†’ "Negative"

If we just take max probability:
â”œâ”€ Article 1: 0.45
â”œâ”€ Article 2: 0.25
â””â”€ Article 3: 0.85

Average: (0.45 + 0.25 + 0.85) / 3 = 0.517 (seems positive)

BUT: Article 3 is VERY NEGATIVE, not positive!
     Raw average doesn't capture this correctly.

SOLUTION: -1.0 TO +1.0 NORMALIZATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Article 1: 0.558  (from formula: calculated earlier)
Article 2: 0.0    (neutral)
Article 3: -0.9   (very negative)

Average: (0.558 + 0.0 + (-0.9)) / 3 = -0.114 (correctly shows negative bias!)

BENEFITS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Captures negative sentiment explicitly
   â”œâ”€ Positive scores: +0.5 to +1.0
   â”œâ”€ Neutral: -0.2 to +0.2
   â””â”€ Negative: -1.0 to -0.5

2. Easy to average across articles
   â”œâ”€ (0.8 + (-0.6) + 0.2) / 3 = 0.13 (correctly weighted)

3. Easy to visualize
   â”œâ”€ Dashboard: -1 (left/red) â€”â€”â†’ 0 (center/gray) â€”â€”â†’ +1 (right/green)

4. Intuitive interpretation
   â”œâ”€ +0.6 = Good (candidate in positive spotlight)
   â”œâ”€ -0.7 = Bad (candidate in negative spotlight)
   â””â”€ 0.0 = Neutral (balanced coverage)

5. Mathematical consistency
   â”œâ”€ Can easily normalize to 0.0-1.0 for dashboards:
   â”‚  displayScore = (sentimentScore + 1.0) / 2.0
   â”‚  = (+0.6 + 1.0) / 2.0 = 0.8 (80% positive)
   â””â”€ Can easily compare entities
```

### 2.4 Why Multiply Confidence in Effective Score?

```
CORE CONCEPT:
â•â•â•â•â•â•â•â•â•â•â•â•â•
effectiveScore = sentimentScore Ã— confidence Ã— relevanceWeight

This means: How much should we trust this signal?
  â”œâ”€ If confidence = 0.0 â†’ Don't trust at all (ignore it)
  â”œâ”€ If confidence = 0.5 â†’ Trust it half (reduce impact)
  â””â”€ If confidence = 1.0 â†’ Trust it fully (use full value)

EXAMPLE 1: HIGH CONFIDENCE - CORRUPTION CHARGES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Article: "Corruption charges filed against candidate"
â”œâ”€ sentimentScore: -0.85 (very negative)
â”œâ”€ confidence: 0.92 (model is 92% sure)
â”œâ”€ relevanceWeight: 1.0 (direct candidate mention)
â”‚
â”œâ”€ WITHOUT multiplying confidence:
â”‚  â””â”€ effectiveScore = -0.85 (full negative impact)
â”‚     Problem: Doesn't account for model uncertainty
â”‚
â””â”€ WITH multiplying confidence:
   â””â”€ effectiveScore = -0.85 Ã— 0.92 Ã— 1.0 = -0.782
      (slightly less than -0.85 because 92% < 100%)
      Result: Use most of the negative signal (reliable)

EXAMPLE 2: MEDIUM CONFIDENCE - ROUTINE NEWS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Article: "Candidate announces new policy"
â”œâ”€ sentimentScore: +0.60 (moderately positive)
â”œâ”€ confidence: 0.45 (model is only 45% sure)
â”œâ”€ relevanceWeight: 1.0 (direct mention)
â”‚
â””â”€ WITH multiplying confidence:
   â””â”€ effectiveScore = +0.60 Ã— 0.45 Ã— 1.0 = +0.27
      (drastically reduced because model is unsure)
      Result: Weight this signal lightly (unreliable)

EXAMPLE 3: LOW CONFIDENCE - CONFUSING TEXT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Article: "Political analysis report with mixed signals"
â”œâ”€ sentimentScore: +0.10 (slightly positive)
â”œâ”€ confidence: 0.28 (model is very confused)
â”œâ”€ relevanceWeight: 0.7 (party mention, not direct)
â”‚
â””â”€ WITH multiplying confidence:
   â””â”€ effectiveScore = +0.10 Ã— 0.28 Ã— 0.7 = +0.0196
      (almost completely ignored)
      Result: This signal has minimal impact

WHY NOT ADD INSTEAD OF MULTIPLY?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
If we added (sentimentScore + confidence):
  Example: 0.75 + 0.45 = 1.20
  Problem 1: Exceeds our -1.0 to +1.0 scale!
  Problem 2: High confidence increases the score artificially
    â””â”€ If model is 90% sure of +0.5, we get +0.5 + 0.9 = +1.4 (wrong!)
  Problem 3: Not mathematically correct for combining probabilities

By multiplying (correct approach):
  Example: 0.75 Ã— 0.45 = 0.3375
  Benefit 1: Stays within -1.0 to +1.0 scale
  Benefit 2: Confidence acts as a "reliability multiplier"
    â””â”€ 0.75 Ã— 0.45 = 0.34 (treat as less reliable)
    â””â”€ 0.75 Ã— 0.90 = 0.68 (treat as more reliable)
  Benefit 3: Mathematically correct (how probability works)

FORMULA INTUITION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
effectiveScore = sentimentScore Ã— confidence Ã— relevanceWeight

Think of it as:
â”œâ”€ sentimentScore = "What is the sentiment?"
â”œâ”€ confidence = "How sure are we?" (0% â†’ 100%)
â””â”€ relevanceWeight = "How relevant is this to the entity?"

Result: Only count sentiment values that are BOTH high AND confident AND relevant
```

### 2.5 Relevance Weights - Different for Each Entity Type

```
RELEVANCE WEIGHT STRATEGY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

When an article mentions MULTIPLE entities, each mention has 
a different weight based on the type of mention:

Direct Mention (Most Relevant):
â”œâ”€ CANDIDATE (subscribed): weight = 1.0
â”‚  â””â”€ Example: "Siddaramaiah announces new policy"
â”œâ”€ CANDIDATE (opponent): weight = 0.95
â”‚  â””â”€ Example: "Opposition candidate visits constituency"
â””â”€ CANDIDATE (party leader): weight = 0.85
   â””â”€ Example: "Congress President speaks about election"

Contextual Mention (Medium Relevant):
â”œâ”€ GEO_UNIT (primary constituency): weight = 0.85
â”‚  â””â”€ Example: "Bangalore South constituency election announced"
â”œâ”€ GEO_UNIT (containing district): weight = 0.70
â”‚  â””â”€ Example: "Bangalore District results are in"
â”œâ”€ GEO_UNIT (parent state): weight = 0.50
â”‚  â””â”€ Example: "Karnataka election commission decision"
â””â”€ GEO_UNIT (other state): weight = 0.15
   â””â”€ Example: "Tamil Nadu election news" (irrelevant to Karnataka candidate)

Party Mention (Least Relevant):
â”œâ”€ Own party: weight = 0.70
â”‚  â””â”€ Example: "Congress launches welfare scheme"
â””â”€ Other party: weight = 0.40
   â””â”€ Example: "BJP criticizes Congress policies"

EXAMPLE ARTICLE WITH MULTIPLE ENTITIES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Title: "Congress launches welfare scheme in Bangalore"

Entity Mentions:
â”œâ”€ CANDIDATE: Siddaramaiah (direct mention in article)
â”œâ”€ GEO_UNIT: Bangalore (primary constituency)
â””â”€ PARTY: Congress (Siddaramaiah's party)

Sentiment: POSITIVE, score = 0.75, confidence = 0.85

Effective Scores FOR THIS SINGLE ARTICLE:
â”œâ”€ For Siddaramaiah pulse:
â”‚  â””â”€ 0.75 Ã— 0.85 Ã— 1.0 = 0.6375 (CANDIDATE weight)
â”‚
â”œâ”€ For Bangalore pulse:
â”‚  â””â”€ 0.75 Ã— 0.85 Ã— 0.85 = 0.541875 (GEO_UNIT weight)
â”‚
â””â”€ For Congress pulse:
   â””â”€ 0.75 Ã— 0.85 Ã— 0.70 = 0.44625 (PARTY weight)

CURRENT SYSTEM STATUS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
You're currently storing:
â”œâ”€ NewsEntityMention {articleId, entityType, entityId, relevanceWeight}
â””â”€ SentimentSignal {geoUnitId, sentimentScore, confidence}

RECOMMENDATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Keep storing all entity mentions with weights.
When calculating pulse, query ALL mentions for the article,
not just the geounit mention, so you can apply correct weights.
```

### 2.6 How SentimentSignal is Used - Core Intelligence Layer

```
SentimentSignal Table Schema:
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ id: Int @id                                                â•‘
â•‘ geoUnitId: Int         â† Which geographic region           â•‘
â•‘ sourceType: DataSourceType (NEWS | ANALYST)               â•‘
â•‘ sourceRefId: Int       â† NewsArticle.id                    â•‘
â•‘ sentiment: SentimentLabel (POSITIVE|NEUTRAL|NEGATIVE)     â•‘
â•‘ sentimentScore: Float  â† -1.0 to +1.0 (normalized)        â•‘
â•‘ confidence: Float      â† 0.0 to 1.0 (model certainty)     â•‘
â•‘ modelVersion: String   â† "kn-en-v1" for audit             â•‘
â•‘ createdAt: DateTime                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USAGE 1: ALERT DETECTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AlertService runs every hour:

Check for Sentiment Spike:
SELECT COUNT(*) FROM SentimentSignal
WHERE geoUnitId IN (subscribed_geos)
  AND createdAt > now() - interval '24 hours'
  AND sentiment = 'NEGATIVE'
  AND confidence >= 0.80

IF count >= 3:
  CREATE Alert {message: "âš ï¸ Negative coverage surge detected!"}

Check for High-Impact Hit:
SELECT * FROM SentimentSignal
WHERE geoUnitId IN (subscribed_geos)
  AND createdAt > now() - interval '24 hours'
  AND sentiment = 'NEGATIVE'
  AND sentimentScore <= -0.70
  AND confidence >= 0.90
ORDER BY sentimentScore DESC LIMIT 1

IF found:
  CREATE Alert {message: "ğŸ”´ Breaking: High-confidence negative article"}

USAGE 2: DAILY STATS AGGREGATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Every night at 11:59 PM:

FOR each GeoUnit:
  SELECT * FROM SentimentSignal
  WHERE geoUnitId = X
    AND DATE(createdAt) = TODAY
  
  Calculate:
  â”œâ”€ avgSentiment = AVG(sentimentScore)
  â”œâ”€ pulseScore = WEIGHTED_AVG(sentimentScore Ã— confidence)
  â””â”€ dominantIssue = MODE(topic_from_articles)
  
  CREATE/UPDATE DailyGeoStats {
    geoUnitId: X,
    date: TODAY,
    avgSentiment: 0.42,
    pulseScore: 0.58,
    dominantIssue: "Development Projects"
  }

USAGE 3: PULSE CALCULATION (7-day trend)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
User requests: GET /analytics/pulse/candidate/123?days=7

SELECT * FROM SentimentSignal
WHERE geoUnitId IN (candidate_constituencies)
  AND createdAt BETWEEN [now-7days, now]

FOR each signal:
  effectiveScore = signal.sentimentScore 
                 Ã— signal.confidence 
                 Ã— getRelevanceWeight(signal.sourceRefId, candidateId)

pulseScore = AVG(effectiveScores)
trend = COMPARE(recentAvg vs baselineAvg)

Return {pulseScore: 0.621, trend: "RISING", topDrivers: [...]}

USAGE 4: DASHBOARD TIME-SERIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
User views: Sentiment trend for Bangalore (30 days)

SELECT * FROM SentimentSignal
WHERE geoUnitId = bangalore_id
  AND createdAt > now() - interval '30 days'
ORDER BY createdAt

Response: Array of {date, sentiment, score, confidence}

Frontend plots line chart showing daily trend
```

---

## ğŸŒ 3. GEO-ATTRIBUTION SYSTEM

### 3.1 The Waterfall Resolver

When a news article is ingested, the system needs to know which geographic region(s) it's relevant to. The **GeoAttributionResolverService** implements a 4-step waterfall strategy.

#### Waterfall Logic
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GEO-ATTRIBUTION WATERFALL RESOLVER                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ARTICLE: "Siddaramaiah launches scheme in Bangalore constituency"
ENTITY MENTIONS in NewsEntityMention:
  â”œâ”€ {entityType: "CANDIDATE", entityId: 123}  â† Siddaramaiah
  â”œâ”€ {entityType: "GEO_UNIT", entityId: 456}   â† Bangalore constituency
  â””â”€ (no PARTY mentions)

RESOLVER STEPS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STEP 1: Check for EXPLICIT GEO_UNIT mentions
  â”œâ”€ Find all mentions where entityType = "GEO_UNIT"
  â”œâ”€ Found: [{entityId: 456}]
  â””â”€ RETURN [456] âœ“ (STOP HERE - explicit trumps everything)

(If no GEO_UNIT mentions, continue...)

STEP 2: Check for CANDIDATE mentions
  â”œâ”€ Find all mentions where entityType = "CANDIDATE"
  â”œâ”€ For each candidate:
  â”‚   â””â”€ Look up CandidateProfile.primaryGeoUnitId
  â”‚       â€¢ Candidate 123 â†’ Profile.primaryGeoUnitId = 456
  â”œâ”€ Collect all resolved GeoUnitIds: [456]
  â””â”€ If found ANY: RETURN [456] âœ“ (STOP HERE)

(If no CANDIDATEâ†’GeoUnit resolution, continue...)

STEP 3: Check for PARTY mentions
  â”œâ”€ Find all mentions where entityType = "PARTY"
  â”œâ”€ For each party:
  â”‚   â””â”€ Default to state-level GeoUnit
  â”‚       â€¢ Example: Congress â†’ Karnataka state (id: 1)
  â”œâ”€ Collect: [1]
  â””â”€ If found: RETURN [1] âœ“ (STOP HERE)

(If ALL else fails, use fallback...)

STEP 4: FALLBACK
  â”œâ”€ Use hardcoded fallback state
  â””â”€ RETURN [1] (Karnataka state GeoUnit)

RESULT for this article: GeoUnitId = 456 (Bangalore constituency)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 3.2 Example Waterfall Scenarios

#### Scenario 1: Explicit Geo-Unit Mention
```
Article: "Election commission announces poll dates for Bangalore"
Entity Mentions:
  - GEO_UNIT: id=456 (Bangalore)

Resolution:
  Step 1: Found GEO_UNIT mention â†’ Return [456]
  
Result: Sentiment â†’ Bangalore (geo level)
```

#### Scenario 2: Candidate Mention (No Explicit Geo)
```
Article: "Siddaramaiah announces 2% DA hike for government employees"
Entity Mentions:
  - CANDIDATE: id=123 (Siddaramaiah)
  
Resolution:
  Step 1: No GEO_UNIT mentions
  Step 2: Found CANDIDATE mention
          â†’ Look up CandidateProfile {candidateId: 123}
          â†’ Find primaryGeoUnitId = 456 (Bangalore)
          â†’ Return [456]
          
Result: Sentiment â†’ Bangalore (via candidate's constituency)
```

#### Scenario 3: Party Mention Only
```
Article: "Congress criticizes GST on agriculture"
Entity Mentions:
  - PARTY: id=789 (Congress)
  
Resolution:
  Step 1: No GEO_UNIT mentions
  Step 2: No CANDIDATE mentions
  Step 3: Found PARTY mention
          â†’ Resolve to state level
          â†’ Return [1] (Karnataka)
          
Result: Sentiment â†’ Karnataka state level
```

#### Scenario 4: No Entity Mentions (Fallback)
```
Article: "Indian election analysis for 2024"
Entity Mentions: [] (empty)

Resolution:
  Step 1-3: No mentions found
  Step 4: Use fallback
          â†’ Return [1] (Karnataka)
          
Result: Sentiment â†’ Karnataka (fallback state)
```

### 3.3 Geographic Hierarchy

```
India
 â”œâ”€ Karnataka (State, level=STATE)
 â”‚   â”œâ”€ Bangalore District (level=DISTRICT)
 â”‚   â”‚   â”œâ”€ Bangalore South Constituency (level=CONSTITUENCY)
 â”‚   â”‚   â”‚   â”œâ”€ Ward 45 (level=WARD)
 â”‚   â”‚   â”‚   â””â”€ Ward 46
 â”‚   â”‚   â”œâ”€ Bangalore North Constituency
 â”‚   â”‚   â””â”€ Bangalore Central Constituency
 â”‚   â”œâ”€ Mysore District
 â”‚   â”‚   â””â”€ Mysore City Constituency
 â”‚   â””â”€ Belagavi District
 â”‚
 â””â”€ (other states)
```

Each GeoUnit has:
```typescript
{
  id: 456,
  name: "Bangalore South",
  level: GeoLevel.CONSTITUENCY,
  parentId: 789,  // parent district
  // Self-referential hierarchy for traversal
}
```

---

## ğŸ“Š 4. DAILY GEO STATS & PULSE ANALYSIS

### 4.1 What is Pulse Score?

**Pulse Score** = Weighted average sentiment over a time period (usually 7 days) representing the overall "health" or "momentum" of a candidate/region.

```
Range: 0.0 to 1.0 (normalized from -1.0 to +1.0)

Score Interpretation:
â”œâ”€ 0.0 to 0.25  â†’ Negative phase (bad coverage, under scrutiny)
â”œâ”€ 0.25 to 0.5  â†’ Cautious/Mixed (balanced coverage with concerns)
â”œâ”€ 0.5          â†’ Neutral (no clear trend, balanced)
â”œâ”€ 0.5 to 0.75  â†’ Positive phase (good coverage, momentum)
â””â”€ 0.75 to 1.0  â†’ Strong positive (excellent coverage, leading)

Example Interpretations:
â”œâ”€ Pulse = 0.20 â†’ "Bad press, candidate in trouble"
â”œâ”€ Pulse = 0.50 â†’ "Balanced coverage, stable position"
â””â”€ Pulse = 0.85 â†’ "Excellent momentum, leading the narrative"
```

### 4.2 Pulse Score Calculation - Complete Example

```
SCENARIO: Calculate 7-day pulse for Siddaramaiah (Candidate ID: 123)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STEP 1: Gather Sentiment Data
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SELECT * FROM SentimentSignal
WHERE geoUnitId IN (
        SELECT primaryGeoUnitId FROM CandidateProfile WHERE candidateId=123
        UNION
        SELECT id FROM GeoUnit WHERE parent=(candidate's state) 
      )
  AND createdAt BETWEEN [now-7days, now]

Found 8 articles:

Date    â”‚ Article Title                              â”‚ Sentiment â”‚ Score â”‚ Conf
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
Jan 8   â”‚ "Siddaramaiah launches dev projects"       â”‚ POSITIVE  â”‚ 0.75  â”‚ 0.85
Jan 8   â”‚ "Congress announces welfare scheme"        â”‚ POSITIVE  â”‚ 0.68  â”‚ 0.82
Jan 7   â”‚ "Report: Policy controversy in state"      â”‚ NEGATIVE  â”‚ -0.60 â”‚ 0.90
Jan 7   â”‚ "Election analysis: Karnataka trends"      â”‚ NEUTRAL   â”‚ 0.10  â”‚ 0.55
Jan 6   â”‚ "CM visits constituency for campaign"      â”‚ POSITIVE  â”‚ 0.72  â”‚ 0.88
Jan 5   â”‚ "Infrastructure projects progressing"      â”‚ POSITIVE  â”‚ 0.80  â”‚ 0.91
Jan 5   â”‚ "Opposition files petition against policy" â”‚ NEGATIVE  â”‚ -0.40 â”‚ 0.75
Jan 4   â”‚ "State election news roundup"              â”‚ NEUTRAL   â”‚ 0.05  â”‚ 0.52

STEP 2: Calculate Effective Scores
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
effectiveScore = sentimentScore Ã— confidence Ã— relevanceWeight

Article 1 (Direct candidate mention):
  Relevance Weight = 1.0 (direct mention)
  effectiveScore = 0.75 Ã— 0.85 Ã— 1.0 = 0.6375

Article 2 (Party mention):
  Relevance Weight = 0.70 (candidate's party, not direct)
  effectiveScore = 0.68 Ã— 0.82 Ã— 0.70 = 0.3903

Article 3 (State-level, affects all):
  Relevance Weight = 0.50 (state-level news)
  effectiveScore = -0.60 Ã— 0.90 Ã— 0.50 = -0.27

Article 4 (General state analysis):
  Relevance Weight = 0.50 (state-level)
  effectiveScore = 0.10 Ã— 0.55 Ã— 0.50 = 0.0275

Article 5 (Direct candidate mention):
  Relevance Weight = 1.0
  effectiveScore = 0.72 Ã— 0.88 Ã— 1.0 = 0.6336

Article 6 (Candidate's work, high confidence):
  Relevance Weight = 1.0
  effectiveScore = 0.80 Ã— 0.91 Ã— 1.0 = 0.728

Article 7 (Opposition to policies, but relevant):
  Relevance Weight = 0.85 (geo mention, candidate affected)
  effectiveScore = -0.40 Ã— 0.75 Ã— 0.85 = -0.255

Article 8 (General state news):
  Relevance Weight = 0.50
  effectiveScore = 0.05 Ã— 0.52 Ã— 0.50 = 0.013

STEP 3: Calculate Raw Pulse Score (Average)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulseRaw = (0.6375 + 0.3903 + (-0.27) + 0.0275 + 0.6336 + 0.728 
            + (-0.255) + 0.013) / 8
         = 2.1448 / 8
         = 0.2681

STEP 4: Normalize to 0.0-1.0 Range
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Why normalize? Raw score is -1.0 to +1.0
              Normalized is 0.0 to 1.0 (easier for dashboards)

Formula: pulseNormalized = (pulseRaw + 1.0) / 2.0

pulseScore = (0.2681 + 1.0) / 2.0
           = 1.2681 / 2.0
           = 0.6341 (63.41%)

STEP 5: Determine Trend (RISING/STABLE/DECLINING)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Recent pulse (last 2 days):   0.68 (Articles 1,2 from Jan 8)
Baseline pulse (7 days):      0.63 (Overall average)

Delta = |0.68 - 0.63| = 0.05

SPIKE_THRESHOLD = 0.15 (15% change minimum)
0.05 < 0.15 â†’ Trend = "STABLE"

(If delta > 0.15 and recent > baseline â†’ "RISING")
(If delta > 0.15 and recent < baseline â†’ "DECLINING")

STEP 6: Identify Top Drivers
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sort articles by absolute effectiveScore and take top 5:

1. Article 6: effectiveScore = +0.728 (Infrastructure projects, high confidence)
2. Article 1: effectiveScore = +0.6375 (Dev projects launch)
3. Article 5: effectiveScore = +0.6336 (CM visit for campaign)
4. Article 2: effectiveScore = +0.3903 (Welfare scheme)
5. Article 3: effectiveScore = -0.27 (Policy controversy)

FINAL PULSE RESPONSE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{
  candidateId: 123,
  candidateName: "Siddaramaiah",
  partyName: "Indian National Congress",
  
  pulseScore: 0.6341,        â† 63.41% positive
  trend: "STABLE",            â† Not significantly changing
  
  articlesAnalyzed: 8,        â† Data points used
  timeWindow: "7 days",       â† Analysis period
  lastUpdated: "2025-01-08T14:32:00Z",
  
  topDrivers: [              â† Most impactful articles
    {
      articleId: 5001,
      headline: "Infrastructure projects progressing well",
      sentiment: "POSITIVE",
      sentimentScore: 0.80,
      confidence: 0.91,
      relevanceWeight: 1.0,
      effectiveScore: 0.728,
      publishedAt: "2025-01-05T10:00:00Z",
      impact: "HIGH"
    },
    {
      articleId: 5000,
      headline: "Siddaramaiah launches development projects",
      sentiment: "POSITIVE",
      sentimentScore: 0.75,
      confidence: 0.85,
      relevanceWeight: 1.0,
      effectiveScore: 0.6375,
      publishedAt: "2025-01-08T08:00:00Z",
      impact: "HIGH"
    },
    // ... more drivers
  ]
}
```

### 4.3 What Makes a Score "High" vs "Low"?

```
BASELINE COMPARISONS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ABSOLUTE SCALE (0.0 to 1.0):
â”œâ”€ 0.0-0.25  = Very poor (critical coverage)
â”œâ”€ 0.25-0.5  = Poor (negative overall)
â”œâ”€ 0.5       = Neutral (balanced)
â”œâ”€ 0.5-0.75  = Good (positive overall)
â””â”€ 0.75-1.0  = Excellent (very positive)

RELATIVE SCALE (Against Peers):
â”œâ”€ Your pulse vs other candidates in same constituency
â”œâ”€ Your pulse vs party average
â””â”€ Your pulse vs previous period

CONTEXTUAL SCALE (Time-based):
â”œâ”€ Pulse during campaign: 0.55-0.75 is normal
â”œâ”€ Pulse before elections: 0.60-0.80 shows momentum
â”œâ”€ Pulse after scandal: 0.20-0.40 is expected
â””â”€ Pulse during routine: 0.45-0.55 is stable

EXAMPLE BENCHMARK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Siddaramaiah pulse = 0.634

Is this HIGH or LOW?
â”œâ”€ Compared to average (0.5):   HIGH (+0.134)
â”œâ”€ Compared to top candidates:  MODERATE (top is 0.78)
â”œâ”€ For election season:         GOOD (expecting 0.60+)
â””â”€ Interpretation: "Candidate is in positive territory with 
                    good momentum but not dominating the narrative"
```

### 4.4 DailyGeoStats Table

```sql
CREATE TABLE DailyGeoStats {
  id: Int @id
  geoUnitId: Int              -- Which geographic region
  date: DateTime @db.Date     -- ISO date (YYYY-MM-DD)
  
  avgSentiment: Float         -- Average sentiment score for the day
  pulseScore: Float           -- 7-day rolling pulse (0.0-1.0)
  dominantIssue: String       -- Most discussed topic ("Infrastructure", "Welfare", etc.)
  
  // Relations
  geoUnit: GeoUnit @relation(fields: [geoUnitId], references: [id])
  
  @@unique([geoUnitId, date]) -- One record per region per day
  @@index([date])             -- Time-range queries across regions
  @@index([geoUnitId])        -- All stats for a region
}
```

### 4.5 Dominant Issue Extraction

```
WHAT IS DOMINANT ISSUE?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
The most frequently discussed topic/theme for a geography on a given day.

Example:
Date: 2025-01-08, GeoUnit: Bangalore

Articles today:
â”œâ”€ "CM launches infrastructure projects" 
â”œâ”€ "Development work speeds up in city"
â”œâ”€ "New roads announced for Bangalore"
â”œâ”€ "Congress welfare scheme launched"
â”œâ”€ "Election voting date announced"

Dominant Issue: "Infrastructure" (appeared 3 times)

HOW TO EXTRACT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

APPROACH 1: SIMPLE KEYWORD FREQUENCY (v0 - Current/Recommended)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Algorithm:
  1. Get all articles for geounit + date
  2. Extract title + summary text
  3. Split into words, remove stop words ("the", "and", "is", etc.)
  4. Count word frequency
  5. Map words to issue categories
  6. Return most frequent issue

Pseudo-code:
```
async computeDominantIssue(geoUnitId, date) {
  // Get articles
  articles = await getArticles(geoUnitId, date)
  
  // Extract + clean text
  allText = articles
    .map(a => a.title + " " + a.summary)
    .join(" ")
    .toLowerCase()
    .split(/\s+/)
  
  // Remove stop words (the, is, and, etc.)
  filtered = allText.filter(w => !STOP_WORDS.has(w))
  
  // Count frequency
  frequency = {}
  for (word of filtered):
    frequency[word] = (frequency[word] || 0) + 1
  
  // Map to issue categories
  issueScores = {}
  for (word, count of frequency):
    if (word in INFRASTRUCTURE_KEYWORDS):
      issueScores["Infrastructure"] += count
    else if (word in WELFARE_KEYWORDS):
      issueScores["Welfare"] += count
    else if (word in ELECTION_KEYWORDS):
      issueScores["Elections"] += count
    // ... more categories
  
  // Return highest scoring issue
  return issueScores.maxKey()  // e.g., "Infrastructure"
}
```

Issue Category Keywords:
```
INFRASTRUCTURE_KEYWORDS:
  roads, highways, bridge, metro, transport, development,
  construction, project, infrastructure, industrial

WELFARE_KEYWORDS:
  welfare, scheme, benefit, assistance, social, healthcare,
  insurance, pension, grant, subsidy

ELECTION_KEYWORDS:
  election, voting, vote, candidate, campaign, polls,
  ballot, constituency, commission, voter

POLITICAL_KEYWORDS:
  congress, bjp, party, political, government, minister,
  policy, law, legislation, amendment

CONTROVERSY_KEYWORDS:
  scandal, corruption, charges, investigation, alleged,
  arrest, controversy, dispute, conflict, issue
```

APPROACH 2: ML-BASED TOPIC EXTRACTION (v1 - Future)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use NLP library (Gensim, spaCy, Hugging Face):
â”œâ”€ Automatically extract topics without manual keywords
â”œâ”€ More accurate for emerging issues
â”œâ”€ Can handle nuanced language
â””â”€ Requires ML model training

APPROACH 3: SENTIMENT-WEIGHTED TOPICS (v2 - Advanced)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Find topic + weight by sentiment impact:

Example:
Topics:
â”œâ”€ Infrastructure: 4 articles, avg sentiment +0.65 â†’ Impact = 2.6 âœ“ High
â”œâ”€ Welfare: 2 articles, avg sentiment +0.58 â†’ Impact = 1.16
â”œâ”€ Voting: 2 articles, avg sentiment -0.2 â†’ Impact = -0.4 (negative)

dominantIssue = "Infrastructure" (highest positive impact)

This shows not just what's most discussed, but what's most impactful.

CURRENT RECOMMENDATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Implement APPROACH 1 (Keyword Frequency) for v0:
â”œâ”€ Simple to implement
â”œâ”€ Sufficient accuracy
â”œâ”€ Fast computation
â””â”€ Can upgrade to ML later
```

### 4.6 Why Use Weighted Scores Instead of Stars?

```
COMPARISON: STARS vs WEIGHTED SCORES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCENARIO: 3 articles about candidate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Article 1: "Amazing development projects launched" â†’ 5 stars (positive)
Article 2: "Corruption charges against candidate" â†’ 1 star (negative)
Article 3: "Mixed analysis of policies" â†’ 3 stars (neutral)

APPROACH A: Using Raw Star Ratings (1-5)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Average = (5 + 1 + 3) / 3 = 3 stars (NEUTRAL)

Problem: This is MISLEADING!
â”œâ”€ Reality: One very positive + one very negative + one neutral
â”œâ”€ Shows as: "Neutral" (balanced)
â”œâ”€ But actually: There's significant conflict
â””â”€ Can't distinguish between:
   â”œâ”€ All 3s (genuinely neutral)
   â”œâ”€ Mix of 1,3,5 (conflicted)
   â””â”€ Mix of 2,3,4 (mostly neutral)

Aggregation issue:
â”œâ”€ If all articles are 3-star â†’ average = 3
â”œâ”€ If mix of 1,3,5 â†’ average = 3
â”œâ”€ Result: Same score for very different situations!

APPROACH B: Using Weighted Scores (-1.0 to +1.0)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Convert stars to weights:
â”œâ”€ 5 stars â†’ +1.0 (most positive)
â”œâ”€ 4 stars â†’ +0.5
â”œâ”€ 3 stars â†’ 0.0 (neutral)
â”œâ”€ 2 stars â†’ -0.5
â”œâ”€ 1 star â†’ -1.0 (most negative)

Example calculation:
â””â”€ Assume confidence adjustments too:
   Article 1: +1.0 Ã— 0.88 = +0.88 (very positive, high confidence)
   Article 2: -1.0 Ã— 0.92 = -0.92 (very negative, high confidence)
   Article 3: +0.0 Ã— 0.55 = 0.0 (neutral)
   
   Average = (+0.88 + (-0.92) + 0.0) / 3 = -0.04/3 = -0.013

Result: "Slightly negative" (ACCURATE!)
â”œâ”€ Correctly shows the conflict
â”œâ”€ Shows negative slightly outweighs positive
â””â”€ Much more useful for decision-making

ADVANTAGE 1: Captures Sentiment Direction
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weighted: Can express negative sentiment (-1 to 0)
Stars: Only express positive (all are 1-5)

Example:
â”œâ”€ Weighted -0.5 = "Bad coverage" âœ“
â”œâ”€ Stars 2 = "2 stars" (what does this mean exactly?) âœ—

ADVANTAGE 2: Enables Mathematical Operations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
With weighted scores:
â”œâ”€ Can multiply by confidence: 0.75 Ã— 0.85 = 0.6375 âœ“
â”œâ”€ Can apply relevance weights: 0.6375 Ã— 0.8 = 0.51 âœ“
â”œâ”€ Can calculate trends: today - yesterday âœ“
â””â”€ Can aggregate hierarchically: district = avg(constituencies) âœ“

With stars:
â”œâ”€ Averaging makes sense: (4 + 5) / 2 = 4.5
â”œâ”€ But what does 4.5 stars mean? (vague)
â””â”€ Can't multiply by confidence meaningfully

ADVANTAGE 3: Better for Visualization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weighted (-1 to +1) maps naturally to:
â”œâ”€ Left (negative, red)
â”œâ”€ Center (neutral, gray)
â””â”€ Right (positive, green)

Can normalize to 0-1 for percentage display:
â””â”€ pulseScore = 0.75 = "75% positive" (intuitive!)

ADVANTAGE 4: More Nuanced Comparability
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Compare two candidates:

Candidate A: 
â”œâ”€ Articles: 5 stars, 1 star, 3 stars
â”œâ”€ Using stars: average = 3 (neutral)
â””â”€ Using weighted: average = 0.0 (neutral but conflicted)

Candidate B:
â”œâ”€ Articles: 3 stars, 3 stars, 3 stars
â”œâ”€ Using stars: average = 3 (neutral)
â””â”€ Using weighted: average = 0.0 (neutral and stable)

Same score but different situations!
â”œâ”€ Weighted shows: A is contested, B is boring
â”œâ”€ Can make different strategic decisions

MATHEMATICAL FOUNDATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Weighted scoring uses:
â”œâ”€ Probability Theory: BERT's 5 probabilities
â”œâ”€ Expected Value Calculation: E(X) = Î£(p_i Ã— x_i)
â”œâ”€ Weighted Averaging: Account for confidence
â””â”€ Linear Normalization: Map to human-readable scale

These are standard statistical concepts used in:
â”œâ”€ Machine Learning
â”œâ”€ Risk Analysis
â”œâ”€ Financial Modeling
â”œâ”€ Quality Assurance
â””â”€ Sentiment Analysis (our use case)

WHEN WOULD YOU USE STARS INSTEAD?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Use stars if:
â”œâ”€ Displaying to non-technical users (intuitive)
â”œâ”€ Don't need to aggregate/compare mathematically
â””â”€ Simple categorical classification needed

But internally:
â”œâ”€ Always use weighted scores for calculations
â”œâ”€ Convert to stars only for user display
â””â”€ Keep the mathematical precision

RECOMMENDATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ“ Use weighted scores (-1 to +1) for all calculations
âœ“ Use stars (1-5) only for user display
âœ“ Never use stars for aggregation/trending
```

---

## ğŸš¨ 5. ALERT SYSTEM

### 5.1 Alert Types

The system automatically detects three types of anomalies and triggers alerts:

#### Type 1: Sentiment Spike
```
DETECTION LOGIC:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Threshold: Î” (change) â‰¥ 0.35
Minimum signals: â‰¥ 3 articles in 24h

Example 1: SPIKE DETECTED
  â”œâ”€ Yesterday's pulse: 0.45
  â”œâ”€ Today's pulse: 0.82
  â”œâ”€ Delta: |0.82 - 0.45| = 0.37 â‰¥ 0.35 âœ“
  â””â”€ Alert: "Positive sentiment spike detected!"

Example 2: NO SPIKE
  â”œâ”€ Baseline pulse: 0.50
  â”œâ”€ Recent pulse: 0.58
  â”œâ”€ Delta: |0.58 - 0.50| = 0.08 < 0.35 âœ—
  â””â”€ No alert

Alert Message:
  "ğŸš¨ Sentiment positive spike detected! Change: +0.37 (3+ articles in last 24h)"
```

#### Type 2: Negative Surge
```
DETECTION LOGIC:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Requirement: â‰¥ 3 negative articles with confidence â‰¥ 0.80 in 24h

Example: SURGE DETECTED
  â”œâ”€ Found articles in last 24h:
  â”‚   â”œâ”€ Article 1: NEGATIVE, confidence=0.85 âœ“
  â”‚   â”œâ”€ Article 2: NEGATIVE, confidence=0.82 âœ“
  â”‚   â”œâ”€ Article 3: NEGATIVE, confidence=0.88 âœ“
  â”‚   â””â”€ Article 4: NEUTRAL, confidence=0.60 âœ—
  â”œâ”€ Count: 3 â‰¥ 3 âœ“
  â””â”€ Alert: "Negative coverage surge detected!"

Alert Message:
  "âš ï¸ Negative coverage surge: 3 high-confidence negative articles detected in last 24 hours"
```

#### Type 3: High-Confidence Hit
```
DETECTION LOGIC:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Requirement: Single article with:
  â€¢ Sentiment: NEGATIVE
  â€¢ sentimentScore â‰¤ -0.70
  â€¢ confidence â‰¥ 0.90

Example: HIT DETECTED
  Article: "Corruption charges filed against candidate"
  â”œâ”€ sentiment: NEGATIVE
  â”œâ”€ sentimentScore: -0.85 â‰¤ -0.70 âœ“
  â”œâ”€ confidence: 0.92 â‰¥ 0.90 âœ“
  â””â”€ Alert: "High-impact negative article detected!"

Alert Message:
  "ğŸ”´ Breaking news alert: High-confidence negative article - 
   'Corruption charges filed against candidate' (confidence: 0.92)"
```

### 5.2 Alert Service Architecture

```typescript
@Injectable()
export class AlertService {
  @Cron(CronExpression.EVERY_HOUR)  // Runs every hour
  async detectAlerts() {
    // 1. Get all candidates with profiles
    candidates = await db.candidateProfile.findMany()
    
    // 2. For each candidate with a subscribed user
    for (profile of candidates) {
      if (profile.userId) {  // Only if user exists
        await checkCandidateAlerts(profile.candidateId, profile.userId)
      }
    }
  }
  
  private async checkCandidateAlerts(candidateId, userId) {
    // Run all three alert checks in parallel
    await Promise.all([
      checkSentimentSpike(candidateId, userId),
      checkNegativeSurge(candidateId, userId),
      checkHighConfidenceHits(candidateId, userId)
    ])
  }
}
```

### 5.3 Alert Storage

```sql
CREATE TABLE Alert {
  id: Int @id
  userId: Int                    -- Who receives the alert
  geoUnitId: Int                 -- Which region/candidate
  type: AlertType                -- SENTIMENT_SPIKE | CONTROVERSY | NEWS_MENTION
  message: String @db.Text       -- "ğŸš¨ Sentiment spike detected..."
  isRead: Boolean @default(false)-- User can mark as read
  
  createdAt: DateTime            -- When alert was generated
  
  user: User                     -- Relation
  @@index([userId, createdAt])   -- User's recent alerts
}
```

---

## â“ 5. FREQUENTLY ASKED QUESTIONS & CLARIFICATIONS

### FAQ 1: Why multiply by confidence? What does "exclude" mean?

```
CONFIDENCE WEIGHTING EXPLAINED:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

When calculating effectiveScore, we multiply:
  effectiveScore = sentimentScore Ã— confidence Ã— relevanceWeight

WHY MULTIPLY BY CONFIDENCE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Confidence = "How sure is the BERT model about this prediction?"

Example Scenario:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Two articles, both show +0.75 sentiment:

Article A: "Development success in Bangalore"
â”œâ”€ BERT Output: [0.01, 0.02, 0.02, 0.10, 0.85]
â”œâ”€ Confidence: 0.85 (85% sure it's very positive)
â”œâ”€ Model is VERY CERTAIN
â””â”€ Should count FULLY: 0.75 Ã— 0.85 = 0.6375

Article B: "Political analysis of mixed policies"
â”œâ”€ BERT Output: [0.20, 0.20, 0.25, 0.20, 0.15]
â”œâ”€ Confidence: 0.25 (25% sure it's 3-star neutral)
â”œâ”€ Model is VERY UNCERTAIN (probabilities all similar)
â””â”€ Should count LESS: 0.75 Ã— 0.25 = 0.1875

Result: Both show +0.75 sentiment score, but Article B has only 29% 
        of the impact of Article A (0.1875 vs 0.6375)
        
This correctly handles uncertainty!

WHAT DOES "EXCLUDE" MEAN?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

You have two exclusion strategies:

SOFT EXCLUDE (Weighted Down):
â”œâ”€ Keep the signal, but reduce its weight
â”œâ”€ How: Multiply by confidence (already doing this!)
â”œâ”€ Example: confidence 0.35 â†’ multiply by 0.35
â”‚           0.75 Ã— 0.35 = 0.2625 (heavily reduced)
â””â”€ When: Use for all signals, no exceptions

HARD EXCLUDE (Remove Completely):
â”œâ”€ Don't include signal in calculations at all
â”œâ”€ How: IF confidence < THRESHOLD THEN skip
â”œâ”€ Threshold: Could be 0.25, 0.30, or 0.40
â””â”€ When: Only for extremely unreliable predictions

RECOMMENDATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Current approach (soft exclude via multiplication) is CORRECT:
â”œâ”€ Keep all signals
â”œâ”€ Let confidence naturally weight them down
â”œâ”€ Only hard exclude if confidence < 0.20 (extremely unreliable)
â””â”€ Result: No data loss, but noisy signals have minimal impact

CONFIDENCE THRESHOLD GUIDE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
0.90+   â†’ Use full weight (0.90 Ã— score = 0.9 Ã— score)
0.70-0.89 â†’ Use full weight (still high confidence)
0.50-0.69 â†’ Reduced weight (0.5-0.69 Ã— score, somewhat uncertain)
0.30-0.49 â†’ Minimal weight (0.3-0.49 Ã— score, quite uncertain)
<0.30   â†’ Consider excluding (OR use 0.20 Ã— score, very uncertain)
```

### FAQ 2: Confusion about Probability & Weights Calculation

```
WHY USE PROBABILITY Ã— WEIGHTS FOR SENTIMENT?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The BERT model gives us: [prob_1star, prob_2star, prob_3star, prob_4star, prob_5star]

Example: [0.02, 0.05, 0.10, 0.45, 0.38]

Question: How do we turn these 5 numbers into ONE sentiment score?

NAIVE APPROACHES (Wrong):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Approach A: Just pick the highest?
â””â”€ argmax([0.02, 0.05, 0.10, 0.45, 0.38]) = 0.45 (4-star)
â””â”€ Problem: Loses the 38% chance of 5-star, loses the 10% neutral chance
â””â”€ Too simplistic, loses information

Approach B: Simple average?
â””â”€ (0.02 + 0.05 + 0.10 + 0.45 + 0.38) / 5 = 0.20
â””â”€ Problem: Not tied to what the stars represent
â””â”€ Meaningless number

CORRECT APPROACH: Weighted Average
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Insight: Each star represents a different sentiment intensity
â”œâ”€ 1-star = -1.0 (most negative)
â”œâ”€ 2-star = -0.5 (negative)
â”œâ”€ 3-star = 0.0 (neutral)
â”œâ”€ 4-star = +0.5 (positive)
â””â”€ 5-star = +1.0 (most positive)

The probabilities tell us: "How likely is each intensity?"

Use expected value formula: E(X) = Î£(probability Ã— value)

sentimentScore = (0.02 Ã— -1.0) + (0.05 Ã— -0.5) + (0.10 Ã— 0.0)
               + (0.45 Ã— +0.5) + (0.38 Ã— +1.0)
               = -0.02 + (-0.025) + 0 + 0.225 + 0.38
               = 0.558

INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0.558 on -1.0 to +1.0 scale = "Strongly leaning positive"

This correctly says:
â”œâ”€ "The model thinks mostly 4-5 star (positive)"
â”œâ”€ "But there's some chance of neutral (10%)"
â”œâ”€ "And small chance of negative (7%)"
â””â”€ "Overall: 55.8% positive"

WHY IS THIS CALCULATION IMPORTANT?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Allows us to:
1. Get ONE number representing overall sentiment
2. Incorporate model's full distribution of belief
3. Weight by confidence (multiply by max probability)
4. Compare across articles (all on same scale)
5. Calculate trends (0.55 today vs 0.42 yesterday)
6. Aggregate hierarchically (candidate = avg(articles))

Without this:
â”œâ”€ Can't compare articles meaningfully
â”œâ”€ Can't detect trends
â”œâ”€ Can't aggregate into pulse scores
â””â”€ Stuck with just "positive/negative/neutral" labels (too coarse)
```

### FAQ 3: Pulse Score Normalization - Why (+1.0) / 2.0?

```
UNDERSTANDING PULSE NORMALIZATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This is NOT time-based. It's a mathematical range transformation.

PROBLEM: Raw Scores are Hard to Interpret
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Raw pulse (before normalization): -1.0 to +1.0

Example scores:
â”œâ”€ -0.5 = Is this -50%? Or is it 50% negative? Confusing!
â”œâ”€ 0.0 = Is this 0%? Or 50%? Ambiguous!
â””â”€ +0.3 = Is this 30%? Or what? Unclear!

Users think in percentages (0-100%), not in bidirectional scales (-100% to +100%).

SOLUTION: Normalize to 0.0-1.0 (0% to 100%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Formula: normalizedScore = (rawScore + 1.0) / 2.0

This is a LINEAR TRANSFORMATION (standard in math/statistics):
  â”œâ”€ Add 1.0: Shift range from [-1, +1] to [0, +2]
  â”œâ”€ Divide by 2.0: Compress range [0, +2] to [0, +1]
  â””â”€ Result: Maps [-1, +1] â†’ [0, 1] (0% to 100%)

NOT TIME-BASED - It's just range mapping!

CONCRETE EXAMPLES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Example 1: Extremely Negative
â””â”€ Raw pulse: -1.0
   Normalized: (-1.0 + 1.0) / 2.0 = 0.0 / 2.0 = 0.0
   Display: "0% positive" or "Completely negative"

Example 2: Perfectly Balanced
â””â”€ Raw pulse: 0.0
   Normalized: (0.0 + 1.0) / 2.0 = 1.0 / 2.0 = 0.5
   Display: "50% positive" or "Perfectly balanced"

Example 3: Moderately Positive (from your calculation)
â””â”€ Raw pulse: 0.2681
   Normalized: (0.2681 + 1.0) / 2.0 = 1.2681 / 2.0 = 0.6341
   Display: "63.41% positive" (good sentiment)

Example 4: Very Positive
â””â”€ Raw pulse: 0.8
   Normalized: (0.8 + 1.0) / 2.0 = 1.8 / 2.0 = 0.9
   Display: "90% positive" (excellent sentiment)

Example 5: Extremely Positive
â””â”€ Raw pulse: 1.0
   Normalized: (1.0 + 1.0) / 2.0 = 2.0 / 2.0 = 1.0
   Display: "100% positive" (perfect sentiment)

MATHEMATICAL CONCEPT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This is called "Min-Max Normalization" or "Feature Scaling"

General formula:
  normalized = (x - min) / (max - min)

For our case where min = -1.0 and max = +1.0:
  normalized = (x - (-1.0)) / (+1.0 - (-1.0))
             = (x + 1.0) / 2.0

This is a standard technique used in:
â”œâ”€ Machine Learning (normalizing features)
â”œâ”€ Statistics (standardization)
â”œâ”€ Data Science (scaling data)
â””â”€ Image Processing (pixel value normalization)

WHY THIS SPECIFIC FORMULA?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Requirement: Map [-1, +1] to [0, 1]

Let's check different values:
â”œâ”€ x = -1.0: (-1.0 + 1.0) / 2.0 = 0.0 âœ“ (left boundary)
â”œâ”€ x = 0.0:  (0.0 + 1.0) / 2.0 = 0.5 âœ“ (middle)
â””â”€ x = +1.0: (+1.0 + 1.0) / 2.0 = 1.0 âœ“ (right boundary)

Linear and monotonic:
â”œâ”€ Larger x â†’ Larger output (preserves ordering)
â”œâ”€ Equal spacing â†’ Equal distance (linear)
â””â”€ Simple and reversible (can reverse if needed)

WHY NOT ALTERNATIVES?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Option A: Just multiply by 0.5?
  0.2681 Ã— 0.5 = 0.13405 (too low!)
  Problem: Negative values stay negative (-0.5 Ã— 0.5 = -0.25 â‰  in [0,1] range!)

Option B: Use sigmoid function?
  sigmoid(x) = 1 / (1 + e^(-x))
  Problem: Non-linear, curves the scale, harder for dashboards

Option C: Use absolute value?
  |0.2681| = 0.2681 (loses sign information!)
  Problem: Can't distinguish -0.5 from +0.5

Option D: (x + 1.0) / 2.0 â† BEST âœ“
  Advantages:
  â”œâ”€ Linear (equal steps = equal changes)
  â”œâ”€ Simple (one operation)
  â”œâ”€ Intuitive (0 = bad, 0.5 = neutral, 1 = good)
  â”œâ”€ Standard in statistics
  â””â”€ Easy to reverse: x = (2.0 Ã— normalized) - 1.0

INTERPRETATION (Two Ways):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Interpretation 1: "Percentage Positive"
â”œâ”€ Normalized score: 0.634
â”œâ”€ Meaning: "63.4% of the sentiment spectrum is positive"
â””â”€ How positive: Moderately positive

Interpretation 2: "Position on Scale"
â”œâ”€ Normalized score: 0.634
â”œâ”€ Scale: 0 (very bad) â”€â”€â”€ 0.5 (neutral) â”€â”€â”€ 1 (very good)
â”œâ”€ Position: Above neutral, trending positive
â””â”€ Relative strength: Good but not excellent

Both interpretations are valid!
```

### FAQ 4: Relevance Weights for Entity Types

```
ARE WE USING RELEVANCE WEIGHTS?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SHORT ANSWER: Conceptually YES, but not fully in database yet.

CURRENT STATUS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… NewsEntityMention table stores multiple entity types per article
âœ… Weights are calculated in our pulse calculation logic
âŒ relevanceWeight field not yet stored in database
âŒ SentimentSignal not linked to specific entity types

WHAT NEEDS TO BE IMPLEMENTED:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Database Schema Update:

CREATE TABLE NewsEntityMention {
  id: Int @id
  articleId: Int
  entityType: EntityType (CANDIDATE | PARTY | GEO_UNIT)
  entityId: Int
  relevanceWeight: Float? â† NEW (currently missing)
  
  @@unique([articleId, entityType, entityId])
  @@index([relevanceWeight]) â† NEW
}

Default Relevance Weights to Populate:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CANDIDATE direct mention:       1.0  (strongest)
CANDIDATE opposition mention:   0.95
GEO_UNIT (primary constituency): 0.85
GEO_UNIT (containing district):  0.70
GEO_UNIT (parent state):         0.50
GEO_UNIT (other state):          0.15
PARTY (same as candidate):       0.70
PARTY (other party):             0.40

USAGE IN PULSE CALCULATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For Siddaramaiah's pulse (7-day):

SELECT signals FROM SentimentSignal s
JOIN NewsArticle a ON s.sourceRefId = a.id
JOIN NewsEntityMention m ON a.id = m.articleId
WHERE m.entityType = 'CANDIDATE' 
  AND m.entityId = 123
  AND s.createdAt > now() - 7 days

For each signal:
  effectiveScore = s.sentimentScore Ã— s.confidence Ã— m.relevanceWeight

Result: Only signals mentioning Siddaramaiah count fully (weight 1.0)
        Signals mentioning Congress get 0.70 weight
        Signals mentioning his constituency get 0.85 weight

RECOMMENDATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Add relevanceWeight field to NewsEntityMention
2. Populate with default values based on entity type
3. Allow admin to override weights per entity
4. Use in all pulse calculations

This gives proper credit based on mention type!
```

### FAQ 5: Sentiment Signal Creation

```
ARE WE CREATING ONE SIGNAL PER ARTICLE?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SHORT ANSWER: Currently, sort of. Recommendation: Should be enhanced.

CURRENT BEHAVIOR:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Article: "Siddaramaiah announces scheme in Bangalore"

Mentions: CANDIDATE (Siddaramaiah), GEO_UNIT (Bangalore), PARTY (Congress)
BERT Result: sentiment=POSITIVE, score=0.75, confidence=0.85

Currently creates:
â”œâ”€ SentimentSignal {
â”‚   geoUnitId: 456 (Bangalore),
â”‚   sourceRefId: 5001,
â”‚   sentiment: POSITIVE,
â”‚   sentimentScore: 0.75,
â”‚   confidence: 0.85
â”‚ }
â”‚
â””â”€ Maybe one more for state level if applicable

PROBLEM WITH CURRENT APPROACH:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Only stores geounit-level sentiment
2. Doesn't track which entity triggered it
3. Can't apply different weights for candidate vs party mention
4. Can't distinguish "Siddaramaiah's pulse" from "Congress party pulse"

RECOMMENDED ENHANCEMENT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Create SentimentSignal for EACH entity mention:

For single article: "Siddaramaiah announces scheme in Bangalore"

Create THREE SentimentSignals (one per entity type):

SentimentSignal 1 (Candidate Pulse):
â”œâ”€ geoUnitId: 456 (Bangalore)
â”œâ”€ sourceRefId: 5001 (article)
â”œâ”€ sourceEntityType: CANDIDATE â† NEW
â”œâ”€ sourceEntityId: 123 â† NEW
â”œâ”€ sourceEntityMentionId: 9001 â† NEW (reference to NewsEntityMention)
â”œâ”€ relevanceWeight: 1.0 â† NEW
â”œâ”€ sentiment: POSITIVE
â”œâ”€ sentimentScore: 0.75
â””â”€ confidence: 0.85

SentimentSignal 2 (Party Pulse):
â”œâ”€ geoUnitId: 456 (Bangalore)
â”œâ”€ sourceRefId: 5001 (article)
â”œâ”€ sourceEntityType: PARTY â† NEW
â”œâ”€ sourceEntityId: 789 â† NEW
â”œâ”€ sourceEntityMentionId: 9002 â† NEW
â”œâ”€ relevanceWeight: 0.70 â† NEW
â”œâ”€ sentiment: POSITIVE
â”œâ”€ sentimentScore: 0.75
â””â”€ confidence: 0.85

SentimentSignal 3 (Geo Pulse):
â”œâ”€ geoUnitId: 456 (Bangalore)
â”œâ”€ sourceRefId: 5001 (article)
â”œâ”€ sourceEntityType: GEO_UNIT â† NEW
â”œâ”€ sourceEntityId: 456 â† NEW
â”œâ”€ sourceEntityMentionId: 9003 â† NEW
â”œâ”€ relevanceWeight: 0.85 â† NEW
â”œâ”€ sentiment: POSITIVE
â”œâ”€ sentimentScore: 0.75
â””â”€ confidence: 0.85

BENEFITS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Each entity type gets proper weight
2. Can query signals by entity type
3. Can calculate entity-specific pulse
4. Can track which mentions matter most
5. Full audit trail of why signal was created

IMPLEMENTATION PRIORITY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Phase 1 (Now): Keep current approach (works for MVP)
Phase 2 (Soon): Add sourceEntityType, sourceEntityId fields
Phase 3 (Next): Create signals for all entity types per article
Phase 4 (Future): Add sentiment signal linking to candidate profiles
```

---

## ğŸ”„ 6. DATA FLOW DIAGRAMS

### 6.1 Complete End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               COMPLETE NEWS-TO-INSIGHT PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HOURLY SCHEDULE (10:00 AM IST)
â”‚
â”œâ”€â†’ NEWS INGESTION (NewsIngestionService.fetchAllNews)
â”‚   â””â”€â†’ For each Candidate/Party/GeoUnit:
â”‚       â””â”€â†’ Build keyword query
â”‚           â””â”€â†’ Query Google News RSS
â”‚               â””â”€â†’ Save articles + create entity links
â”‚                   â””â”€â†’ ASYNC: Trigger sentiment analysis
â”‚                       â””â”€â†’ SentimentAnalysisService.analyze()
â”‚
â”œâ”€â†’ SENTIMENT ANALYSIS (SentimentAnalysisService)
â”‚   â””â”€â†’ For each article:
â”‚       â”œâ”€â†’ Call Python BERT model
â”‚       â”‚   â””â”€â†’ Returns: {label, score, confidence}
â”‚       â”‚
â”‚       â”œâ”€â†’ Resolve GeoUnit(s) via waterfall resolver
â”‚       â”‚   â””â”€â†’ Priority: Explicit GEO â†’ CANDIDATE â†’ PARTY â†’ FALLBACK
â”‚       â”‚
â”‚       â””â”€â†’ Create SentimentSignal(s)
â”‚           â””â”€â†’ Store in DB with geoUnitId + sentiment score
â”‚
â”œâ”€â†’ DAILY STATS COMPUTATION (Scheduled nightly 11:59 PM)
â”‚   â””â”€â†’ For each GeoUnit:
â”‚       â””â”€â†’ Aggregate all sentiment signals from the day
â”‚           â””â”€â†’ Calculate: avgSentiment, pulseScore, dominantIssue
â”‚               â””â”€â†’ Create/update DailyGeoStats record
â”‚
â”œâ”€â†’ PULSE CALCULATION (On-demand or cached)
â”‚   â””â”€â†’ For requested candidate:
â”‚       â”œâ”€â†’ Fetch all relevant sentiment signals (7-day window)
â”‚       â”œâ”€â†’ Calculate effective scores
â”‚       â”‚   â””â”€â†’ effectiveScore = sentimentScore Ã— confidence Ã— relevanceWeight
â”‚       â”œâ”€â†’ Compute pulse = AVG(effectiveScores)
â”‚       â”œâ”€â†’ Determine trend (RISING/STABLE/DECLINING)
â”‚       â””â”€â†’ Identify top drivers (highest impact articles)
â”‚
â”œâ”€â†’ ALERT DETECTION (Hourly background job)
â”‚   â””â”€â†’ For each candidate with subscribed user:
â”‚       â”œâ”€â†’ Check Sentiment Spike (Î” â‰¥ 0.35)
â”‚       â”œâ”€â†’ Check Negative Surge (â‰¥3 articles, confidence â‰¥0.80)
â”‚       â””â”€â†’ Check High-Confidence Hit (score â‰¤-0.70, confidence â‰¥0.90)
â”‚           â””â”€â†’ Create Alert record if triggered
â”‚
â””â”€â†’ API RESPONSE TO DASHBOARD
    â””â”€â†’ User requests candidate pulse
        â”œâ”€â†’ Return: pulseScore, trend, topDrivers, sentiment timeline
        â””â”€â†’ Dashboard displays with graphs and notifications
```

### 6.2 Request-Response Cycle

```
USER REQUEST: GET /api/analytics/pulse/candidate/123?days=7
â”‚
â”œâ”€â†’ BACKEND (NestJS) receives request
â”‚   â””â”€â†’ AnalyticsController.getPulseTrend(candidateId=123, days=7)
â”‚       â””â”€â†’ CandidatePulseService.calculatePulse(123, 7)
â”‚           â”‚
â”‚           â”œâ”€â†’ Step 1: Fetch candidate + profile
â”‚           â”‚   â””â”€â†’ Get candidate.partyId, profile.primaryGeoUnitId
â”‚           â”‚
â”‚           â”œâ”€â†’ Step 2: Query SentimentSignal entities
â”‚           â”‚   â””â”€â†’ WHERE createdAt BETWEEN [now-7days, now]
â”‚           â”‚           AND newsArticle.entityMentions has candidate/party/geo
â”‚           â”‚
â”‚           â”œâ”€â†’ Step 3: Calculate scores
â”‚           â”‚   â””â”€â†’ FOR each signal:
â”‚           â”‚       â””â”€â†’ effectiveScore = score Ã— confidence Ã— relevance
â”‚           â”‚
â”‚           â”œâ”€â†’ Step 4: Compute metrics
â”‚           â”‚   â”œâ”€â†’ pulseScore = AVG(effectiveScores)
â”‚           â”‚   â”œâ”€â†’ trend = compare recent vs baseline
â”‚           â”‚   â””â”€â†’ topDrivers = top 5 articles by impact
â”‚           â”‚
â”‚           â””â”€â†’ RETURN PulseData object
â”‚
â””â”€â†’ FRONTEND (React) receives response
    â””â”€â†’ Display:
        â”œâ”€â†’ Large pulse score card (0.621)
        â”œâ”€â†’ Trend indicator (â†‘ RISING)
        â”œâ”€â†’ Articles count (8 analyzed)
        â”œâ”€â†’ Top driver articles with sentiment badges
        â””â”€â†’ Time-series graph of pulse over 7 days
```

---

## ğŸ“ˆ 7. REAL-WORLD EXAMPLE

### Complete Example: Tracking Siddaramaiah

**Scenario:** Candidate "Siddaramaiah" (Congress) in Bangalore South constituency

#### Day 1: News Ingestion (10:00 AM)
```
SCHEDULE: Hourly news fetch job runs
â”‚
â”œâ”€â†’ Find all active candidates
â”‚   â””â”€â†’ Include Candidate: Siddaramaiah (id=123, party_id=789)
â”‚
â”œâ”€â†’ Build search query for Siddaramaiah
â”‚   â””â”€â†’ Query: ("Siddaramaiah" OR "Siddu") AND (election OR vote OR campaign...)
â”‚
â”œâ”€â†’ Query Google News RSS
â”‚   â””â”€â†’ Found 3 articles:
â”‚
â”‚       Article 1: "Siddaramaiah announces infrastructure project for Bangalore"
â”‚       â€¢ URL: news.google.com/...
â”‚       â€¢ Published: 2025-01-08 08:00 AM
â”‚       â€¢ Source: The Hindu
â”‚
â”‚       Article 2: "Congress launches welfare scheme - Siddaramaiah leads"
â”‚       â€¢ URL: news.google.com/...
â”‚       â€¢ Published: 2025-01-08 09:15 AM
â”‚       â€¢ Source: Deccan Herald
â”‚
â”‚       Article 3: "Election analysis: Siddaramaiah's chances in Bangalore"
â”‚       â€¢ URL: news.google.com/...
â”‚       â€¢ Published: 2025-01-08 09:45 AM
â”‚       â€¢ Source: The News Minute
â”‚
â”œâ”€â†’ For each article:
â”‚   â”œâ”€â†’ Check dedup: URL not in DB â†’ CREATE NewsArticle
â”‚   â”œâ”€â†’ Create NewsEntityMention {articleId, entityType: CANDIDATE, entityId: 123}
â”‚   â””â”€â†’ TRIGGER async sentiment analysis
â”‚
â””â”€â†’ Articles saved, sentiment jobs queued
```

#### Parallel: Sentiment Analysis
```
PYTHON SERVICE processes 3 articles asynchronously

Article 1: "Siddaramaiah announces infrastructure project..."
  â”œâ”€â†’ BERT Model processes full text
  â”œâ”€â†’ Output probabilities: [0.02, 0.05, 0.15, 0.43, 0.35]
  â”œâ”€â†’ Label: POSITIVE (index 3 has 0.43)
  â”œâ”€â†’ Score: 0.02Ã—(-1) + 0.05Ã—(-0.5) + 0.15Ã—0 + 0.43Ã—0.5 + 0.35Ã—1 = 0.515
  â””â”€â†’ Confidence: 0.43
  
  Store SentimentSignal:
  â€¢ geoUnitId: 456 (Bangalore South, resolved via CandidateProfile.primaryGeoUnitId)
  â€¢ sentiment: POSITIVE
  â€¢ sentimentScore: 0.515
  â€¢ confidence: 0.43
  â€¢ modelVersion: "kn-en-v1"

Article 2: "Congress launches welfare scheme..."
  â”œâ”€â†’ BERT: [0.01, 0.04, 0.12, 0.50, 0.33]
  â”œâ”€â†’ Label: POSITIVE
  â”œâ”€â†’ Score: 0.535
  â””â”€â†’ Confidence: 0.50
  
  Store SentimentSignal (with partyId=789 in entity mentions)
  â€¢ geoUnitId: 1 (Karnataka state, resolved via Party â†’ state fallback)
  â€¢ sentiment: POSITIVE
  â€¢ sentimentScore: 0.535
  â€¢ confidence: 0.50

Article 3: "Election analysis: Siddaramaiah's chances..."
  â”œâ”€â†’ BERT: [0.05, 0.10, 0.35, 0.30, 0.20]
  â”œâ”€â†’ Label: NEUTRAL
  â”œâ”€â†’ Score: 0.075
  â””â”€â†’ Confidence: 0.35
  
  Store SentimentSignal
  â€¢ geoUnitId: 456 (Bangalore South)
  â€¢ sentiment: NEUTRAL
  â€¢ sentimentScore: 0.075
  â€¢ confidence: 0.35
```

#### Day 2: Pulse Calculation (User requests dashboard)
```
USER: GET /api/analytics/pulse/candidate/123?days=7

BACKEND CALCULATES:
  â”œâ”€â†’ Fetch signals for Siddaramaiah in last 7 days
  â”‚   â””â”€â†’ Found 3 signals (from above)
  â”‚
  â”œâ”€â†’ Calculate effective scores
  â”‚   â”œâ”€â†’ Signal 1: 0.515 Ã— 0.43 Ã— 1.0 (direct mention) = 0.221
  â”‚   â”œâ”€â†’ Signal 2: 0.535 Ã— 0.50 Ã— 0.7 (party mention) = 0.187
  â”‚   â””â”€â†’ Signal 3: 0.075 Ã— 0.35 Ã— 0.8 (geo mention) = 0.021
  â”‚
  â”œâ”€â†’ Calculate pulse
  â”‚   â””â”€â†’ pulseScore = (0.221 + 0.187 + 0.021) / 3 = 0.476
  â”‚
  â”œâ”€â†’ Determine trend
  â”‚   â””â”€â†’ Today: 0.476, Baseline (7d): 0.420
  â”‚   â””â”€â†’ Delta: 0.056 < 0.15 â†’ STABLE
  â”‚
  â””â”€â†’ Get top drivers
      â””â”€â†’ [Article 1 (0.221), Article 2 (0.187), Article 3 (0.021)]

RESPONSE:
{
  "candidateId": 123,
  "candidateName": "Siddaramaiah",
  "partyName": "Indian National Congress",
  "pulseScore": 0.476,
  "trend": "STABLE",
  "articlesAnalyzed": 3,
  "topDrivers": [
    {
      "headline": "Siddaramaiah announces infrastructure project...",
      "sentiment": "POSITIVE",
      "sentimentScore": 0.515,
      "confidence": 0.43,
      "effectiveScore": 0.221
    },
    ...
  ]
}

FRONTEND DISPLAYS:
  â”œâ”€â†’ Large card: "0.476" (47.6% positive sentiment)
  â”œâ”€â†’ Trend badge: "STABLE" (no major change)
  â”œâ”€â†’ Articles analyzed: "3 in 7 days"
  â”œâ”€â†’ Top driver: Article 1 with positive badge
  â””â”€â†’ Timeline graph showing pulse trend
```

#### Hourly Alert Check (11:00 AM - 1 hour after ingestion)
```
ALERT SERVICE runs hourly check for all candidates with users

For Siddaramaiah (candidateId=123):
  â”œâ”€â†’ Check Sentiment Spike
  â”‚   â”œâ”€â†’ Today's pulse: 0.476 (from 3 articles)
  â”‚   â”œâ”€â†’ 7-day baseline: 0.420
  â”‚   â”œâ”€â†’ Delta: 0.056 < 0.35 âœ—
  â”‚   â””â”€â†’ NO ALERT
  â”‚
  â”œâ”€â†’ Check Negative Surge
  â”‚   â”œâ”€â†’ Negative articles in 24h: 0
  â”‚   â”œâ”€â†’ Minimum required: 3
  â”‚   â””â”€â†’ NO ALERT
  â”‚
  â””â”€â†’ Check High-Confidence Hit
      â”œâ”€â†’ Single negative articles with score â‰¤ -0.70 and confidence â‰¥ 0.90: 0
      â””â”€â†’ NO ALERT

RESULT: No alerts triggered (positive news day)
```

---

## ğŸ’¾ 8. DATA STORAGE & INDEXING

### 8.1 Key Database Tables

| Table | Purpose | Key Fields | Indexes |
|-------|---------|-----------|---------|
| NewsArticle | Store articles | title, sourceUrl, publishedAt, status | status, publishedAt |
| NewsEntityMention | Link articles to entities | articleId, entityType, entityId | articleId, entityType |
| SentimentSignal | Sentiment data per GeoUnit | geoUnitId, sentiment, sentimentScore, confidence | geoUnitId + createdAt |
| DailyGeoStats | Daily aggregated stats | geoUnitId, date, avgSentiment, pulseScore | date, geoUnitId |
| Alert | User alerts | userId, type, message, isRead | userId, createdAt |
| CandidateProfile | Candidate â†’ GeoUnit mapping | candidateId, primaryGeoUnitId, partyId | candidateId |

### 8.2 Query Performance

```sql
-- Fast: Get recent signals for a candidate's constituency
SELECT * FROM SentimentSignal
WHERE geoUnitId = 456
  AND createdAt >= NOW() - INTERVAL '7 days'
ORDER BY createdAt DESC;

-- Fast: Get daily stats across time period
SELECT * FROM DailyGeoStats
WHERE date BETWEEN '2025-01-01' AND '2025-01-08'
  AND geoUnitId IN (456, 789, 1)
ORDER BY date DESC;

-- Fast: Get recent articles for moderation
SELECT * FROM NewsArticle
WHERE status = 'PENDING'
ORDER BY createdAt DESC
LIMIT 50;
```

---

## ğŸ” 9. Error Handling & Resilience

### 9.1 Sentiment Analysis Failure
```typescript
// If Python service is down:
try {
  response = await httpService.post(ANALYSIS_SERVICE_URL, ...)
} catch (error) {
  // Non-blocking failure - article is saved, sentiment is skipped
  logger.error(`Sentiment analysis failed: ${error.message}`)
  // Continue ingestion process
}

// Result: Article saved but no sentiment signal created
// Admin can manually review or reprocess later
```

### 9.2 News Fetch Failure
```typescript
// If Google News is down:
try {
  feed = await parser.parseURL(feedUrl)
} catch (error) {
  logger.error(`Failed to fetch news for entity ${entityId}`)
}

// Result: Job skips this entity, continues with others
// Next hourly run will retry
```

### 9.3 Deduplication Handling
```typescript
// If article URL already exists:
existing = await prisma.newsArticle.findFirst({
  where: { sourceUrl: link }
})

if (existing) {
  // Check if entity is already linked
  existingLink = await prisma.newsEntityMention.findFirst({
    where: {articleId: existing.id, entityType, entityId}
  })
  
  if (!existingLink) {
    // New entity mention for existing article - link it
    await prisma.newsEntityMention.create(...)
  }
  return; // Exit - don't create duplicate
}
```

---

## ğŸ¯ 10. Summary of Key Metrics

### Sentiment Score
- **Range**: -1.0 (extremely negative) to +1.0 (extremely positive)
- **Meaning**: Overall tone of an article
- **Calculation**: Weighted average of BERT model's 5-star prediction probabilities

### Confidence Score
- **Range**: 0.0 to 1.0
- **Meaning**: How certain the BERT model is about its sentiment prediction
- **Usage**: Multiplied into effective score to weight uncertain predictions lower

### Pulse Score
- **Range**: 0.0 to 1.0 (normalized from -1.0 to +1.0)
- **Meaning**: Overall health/momentum of a candidate/region
- **Calculation**: Weighted average of sentiment signals, considering:
  - Sentiment score
  - Confidence level
  - Relevance weight (direct vs indirect mentions)
  - Time decay (recent news weighted more)

### Trend
- **RISING**: Recent pulse > baseline by â‰¥15%
- **STABLE**: Change < 15%
- **DECLINING**: Recent pulse < baseline by â‰¥15%

### Alert Types
1. **Sentiment Spike**: Î” â‰¥ 0.35 with â‰¥3 signals in 24h
2. **Negative Surge**: â‰¥3 negative articles with confidence â‰¥0.80
3. **High-Confidence Hit**: Single article with score â‰¤-0.70 and confidence â‰¥0.90

---

## ğŸš€ 11. Technology Stack

- **Frontend**: React with TypeScript
- **Backend API**: NestJS (Node.js)
- **NLP Service**: FastAPI (Python) with Hugging Face BERT
- **Database**: PostgreSQL with Prisma ORM
- **Scheduling**: NestJS Schedule (@Cron)
- **News Source**: Google News RSS API
- **Language Detection**: Python langdetect library
- **ML Model**: Pretrained BERT (multilingual-capable)

---

## ğŸ“Š 12. Example Data Flow Timeline

```
08:00 AM - News published: "Siddaramaiah announces infrastructure project"
   â†“
08:30 AM - Google News indexes the article
   â†“
10:00 AM - Hourly news ingestion job runs
   â”œâ”€â†’ Fetches Google News RSS for "Siddaramaiah"
   â”œâ”€â†’ Finds article
   â”œâ”€â†’ Creates NewsArticle (id=5001)
   â”œâ”€â†’ Creates NewsEntityMention (articleId=5001, entityType=CANDIDATE, entityId=123)
   â””â”€â†’ Triggers sentiment analysis
      â†“
10:02 AM - Python service analyzes sentiment
   â”œâ”€â†’ BERT predicts: POSITIVE, score=0.515, confidence=0.43
   â”œâ”€â†’ Geo resolver â†’ primaryGeoUnitId = 456 (Bangalore South)
   â””â”€â†’ Creates SentimentSignal (geoUnitId=456, sentiment=POSITIVE, score=0.515)
      â†“
10:05 AM - Dashboard user requests pulse for Siddaramaiah
   â”œâ”€â†’ Backend queries SentimentSignal for last 7 days
   â”œâ”€â†’ Finds 3 signals (this article + previous articles)
   â”œâ”€â†’ Calculates pulseScore = 0.476
   â””â”€â†’ Returns PulseData to frontend
      â†“
10:06 AM - Frontend displays
   â”œâ”€â†’ Pulse score: 0.476
   â”œâ”€â†’ Trend: STABLE
   â”œâ”€â†’ Top driver: Article (0.515 score)
   â””â”€â†’ Timeline graph updated
      â†“
11:00 AM - Hourly alert check job runs
   â”œâ”€â†’ Compares today (0.476) vs 7-day baseline (0.420)
   â”œâ”€â†’ Delta: 0.056 < 0.35 threshold
   â””â”€â†’ No alert triggered
      â†“
11:59 PM - Daily stats computation job runs
   â”œâ”€â†’ Aggregates all signals for the day across all regions
   â”œâ”€â†’ Creates/updates DailyGeoStats entries
   â””â”€â†’ Available for next day's trend analysis
```

---

## âœ… Key Takeaways

1. **News Ingestion** is automated hourly, using entity keywords to fetch relevant articles
2. **Sentiment Analysis** uses BERT deep learning to score articles from -1.0 to +1.0
3. **Confidence** indicates how certain the model is (0.0-1.0)
4. **Geo-Attribution** uses a waterfall strategy to assign articles to geographic regions
5. **Pulse Score** is a weighted composite of sentiment, confidence, and relevance
6. **Alerts** detect anomalies like sentiment spikes, negative surges, and high-impact articles
7. **Daily Stats** aggregate sentiment into time-series data for trend analysis
8. **Resilience** ensures failures don't block the pipeline - all operations are non-blocking

This system enables real-time political intelligence tracking with automatic sentiment monitoring and anomaly detection!
